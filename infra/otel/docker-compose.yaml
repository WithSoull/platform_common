services:
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.123.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"] # Path to collector config
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml # Mount collector config
    ports:
      - "${OTEL_GRPC_PORT}:4317" # OTLP gRPC port
      - "${OTEL_HTTP_PORT}:4318" # OTLP HTTP port
      - "8888:8888" # Collector metrics endpoint
      - "8889:8889" # zpages debug endpoint
      - "4317:4317" # OTLP endpoint for receiving logs 

    restart: unless-stopped
    # Auto-restart on failure, not on manual stop
    networks:
      - messenger-network
    env_file: .env

    # Connect to the shared microservices network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--spider", "http://localhost:8888/metrics"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    depends_on:
      jaeger:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      prometheus:
        condition: service_healthy

  jaeger:
    image: jaegertracing/jaeger:2.6.0
    container_name: jaeger
    ports:
      - "${JAEGER_UI_PORT}:16686" # Jaeger UI port
    env_file: .env
    environment:
      - COLLECTOR_OTLP_ENABLED=true # Enable OpenTelemetry protocol
      - COLLECTOR_OTLP_HTTP_ENABLED=true # Accept OTLP over HTTP (4318)
      - COLLECTOR_OTLP_GRPC_ENABLED=true # Accept OTLP over gRPC (4317)
      - STORAGE_TYPE=badger # Embedded key-value storage
      - BADGER_EPHEMERAL=false # Persist data across restarts
      - BADGER_DIRECTORY_VALUE=/badger/data # Values (spans, tags, logs)
      - BADGER_DIRECTORY_KEY=/badger/key # Indexes for fast search
      - BADGER_MAINTENANCE_INTERVAL=5m # Compaction/cleanup interval
      - BADGER_TTL=168h # Data retention: 7 days
      - QUERY_BASE_PATH=/jaeger # Base path when behind reverse proxy
    volumes:
      - jaeger_data:/badger/data # Trace data volume
      - jaeger_key:/badger/key # Index data volume
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "localhost:${JAEGER_UI_PORT}"] # UI availability check
      interval: 5s # Check interval
      timeout: 3s # Check timeout
      retries: 3 # Retry attempts
      start_period: 5s # Delay before starting checks
    restart: unless-stopped
    networks:
      - messenger-network

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.4
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - cluster.name=es-docker-cluster
    
    volumes:
      - es_data:/usr/share/elasticsearch/data
    
    ports:
      - "9200:9200"
    
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200/_cluster/health >/dev/null"]
      interval: 10s
      timeout: 5s 
      retries: 20
      start_period: 30s
    
    restart: unless-stopped

    networks:
      - messenger-network

  kibana:
    image: docker.elastic.co/kibana/kibana:9.0.4
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - XPACK_SECURITY_ENABLED=false
      - xpack.encryptedSavedObjects.encryptionKey=REPLACE_WITH_32+_CHAR_STRING
    
    depends_on:
      elasticsearch:
        condition: service_healthy
    
    ports:
      - "5601:5601"
    
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:5601/api/status >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 30s
    
    restart: unless-stopped
    networks:
      - messenger-network

  kibana-init: # This is special container for auto creating data view in Kibana
    image: alpine:3.21
    container_name: kibana-init
    depends_on:
      kibana:
        condition: service_healthy
    
    command: 
      - sh
      - -c
      - |
        set -e # stop scipt after first error
        
        echo "Installing curl and jq..."
        apk add --no-cache curl jq >/dev/null 2>&1
        
        echo "Waiting for Kibana..."
        # max 60 attempts with delay 2 seconds
        for i in $$(seq 1 60); do
          if curl -sSf http://kibana:5601/api/status >/dev/null; then break; fi
          sleep 2
        done
        
        echo "Creating Data View logs* ..."
        RESPONSE=$$(curl -sS -X POST http://kibana:5601/api/data_views/data_view \
          -H "kbn-xsrf: true" \
          -H "Content-Type: application/json" \
          --data '{"data_view":{"title":"logs*","name":"Logs","timeFieldName":"@timestamp","allowNoIndex":true},"override":true}' || echo "{}")
        
        echo "Response: $$RESPONSE"
        DATA_VIEW_ID=$$(echo "$$RESPONSE" | jq -r ".data_view.id // empty")
        echo "Data View ID: $$DATA_VIEW_ID"
        
        # Making Data View default in Kibana
        if [ -n "$$DATA_VIEW_ID" ]; then
          echo "Setting as default data view..."
          curl -sS -X POST http://kibana:5601/api/data_views/default \
            -H "kbn-xsrf: true" \
            -H "Content-Type: application/json" \
            -d "{\"data_view_id\": \"$$DATA_VIEW_ID\", \"force\": true}" || true
          echo "Default data view set successfully."
        else
          echo "Failed to get data view ID"
        fi
        echo "Done."
    
    networks:
      - messenger-network
    
    restart: "no"

  prometheus:
    image: prom/prometheus:v3.3.1
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml # mount prometheus config
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--web.enable-remote-write-receiver'
    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--spider", "http://localhost:9090/-/healthy" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

    restart: unless-stopped

    networks:
      - messenger-network

  grafana:
    image: grafana/grafana:12.0.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy

    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--spider", "http://localhost:3000/api/health" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

    restart: unless-stopped

    networks:
      - messenger-network

networks:
  messenger-network:
    external: true

volumes: 
  jaeger_data: 
  jaeger_key: # Named volume for Jaeger indexes

  es_data: # Elacstic search volume for storing logs

  prometheus_data:
  grafana_data: 
